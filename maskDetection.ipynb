{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_images_and_labels(image_dir, label_dir, image_size=(128, 128)):\n",
    "    images = [] \n",
    "    labels = []\n",
    "\n",
    "    #go through the image files in the folder\n",
    "    for filename in os.listdir(image_dir):\n",
    "        if filename.endswith('.jpeg'):\n",
    "            #make the path for the image and that images label\n",
    "            img_path = os.path.join(image_dir, filename)\n",
    "            label_path = os.path.join(label_dir, filename.replace('.jpeg', '.txt'))\n",
    "\n",
    "            # Load and resize image \n",
    "            img = imread(img_path) \n",
    "            #images might come in many sizes, making them consitent makes feature extraction easier \n",
    "            #also normalises to help in our model development\n",
    "            img_resized = resize(img, image_size, anti_aliasing=True) \n",
    "            images.append(img_resized)  # add to the list \n",
    "\n",
    "            # Read label\n",
    "            with open(label_path, 'r') as f:\n",
    "                label = int(f.read().strip()) #read the label and convert it to an interger\n",
    "            labels.append(label) # add to the list \n",
    "\n",
    "    #converts the two lists, image and label into numpy arrays \n",
    "    return np.array(images, dtype=np.float32), np.array(labels, dtype=np.int64)\n",
    "\n",
    "def prepare_datasets(\n",
    "    train_image_path, train_label_path,\n",
    "    test_image_path, test_label_path,\n",
    "    image_size=(128, 128),\n",
    "    val_split=0.2,\n",
    "    seed=42\n",
    "):\n",
    "    # Load the whole training set both labels and images, they are resized and normalised \n",
    "    X_train_full, y_train_full = load_images_and_labels(train_image_path, train_label_path, image_size)\n",
    "    \n",
    "    #same with the test set \n",
    "    X_test, y_test = load_images_and_labels(test_image_path, test_label_path, image_size)\n",
    "\n",
    "    # Split training data into train validation, standard split\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_full, y_train_full, test_size=val_split, random_state=seed, stratify=y_train_full #ensure smae label distribution with stratify\n",
    "    )\n",
    "\n",
    "    #sanity check\n",
    "    print(f\"Train: {X_train.shape}, {y_train.shape}\")\n",
    "    print(f\"Validation: {X_val.shape}, {y_val.shape}\")\n",
    "    print(f\"Test: {X_test.shape}, {y_test.shape}\")\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load training and test data, resize, normalise, split the data into the sets, and returns as numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (1915, 128, 128, 3), (1915,)\n",
      "Validation: (479, 128, 128, 3), (479,)\n",
      "Test: (458, 128, 128, 3), (458,)\n"
     ]
    }
   ],
   "source": [
    "train_image_path = '/Users/david/Library/Mobile Documents/com~apple~CloudDocs/Documents/Documents – David’s MacBook Pro/university/year 3/Computer vision/cw/CV2024_CW_Dataset/train/images'\n",
    "train_label_path = '/Users/david/Library/Mobile Documents/com~apple~CloudDocs/Documents/Documents – David’s MacBook Pro/university/year 3/Computer vision/cw/CV2024_CW_Dataset/train/labels'\n",
    "test_image_path = '/Users/david/Library/Mobile Documents/com~apple~CloudDocs/Documents/Documents – David’s MacBook Pro/university/year 3/Computer vision/cw/CV2024_CW_Dataset/test/images'\n",
    "test_label_path = '/Users/david/Library/Mobile Documents/com~apple~CloudDocs/Documents/Documents – David’s MacBook Pro/university/year 3/Computer vision/cw/CV2024_CW_Dataset/test/labels'\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = prepare_datasets(\n",
    "    train_image_path, train_label_path,\n",
    "    test_image_path, test_label_path,\n",
    "    image_size=(128, 128)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# class encompassing all of the evaluation metrics i will use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class ModelEvaluator:\n",
    "    def __init__(self, class_names=[\"No Mask\", \"Mask\", \"Incorrect\"]):\n",
    "        # instead of having the 0,1,2 labels usd actual descriptors for clarity \n",
    "        self.class_names = class_names\n",
    "\n",
    "    def evaluate(self, y_true, y_pred, model_name=\"Model\", verbose=True):\n",
    "      \n",
    "        results = {}\n",
    "\n",
    "        # accuracy score \n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        results['accuracy'] = acc\n",
    "\n",
    "        # Macro = equally weighs all classes (sensitive to minority classes)\n",
    "        results['precision_macro'] = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "        results['recall_macro'] = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "        results['f1_macro'] = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "\n",
    "        # Micro = globally computed over all samples for class imbalance)\n",
    "        results['precision_micro'] = precision_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "        results['recall_micro'] = recall_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "        results['f1_micro'] = f1_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "\n",
    "        # Weighted = like macro but adjusts for class imbalance by using class frequencies\n",
    "        results['precision_weighted'] = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "        results['recall_weighted'] = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "        results['f1_weighted'] = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\nEvaluation results for {model_name}:\")\n",
    "            print(f\"Accuracy: {acc:.4f}\")\n",
    "            print(f\"F1 Score (macro): {results['f1_macro']:.4f}\")\n",
    "            print(f\"F1 Score (weighted): {results['f1_weighted']:.4f}\")\n",
    "            print(\"\\nClassification Report:\")\n",
    "            print(classification_report(y_true, y_pred, target_names=self.class_names, zero_division=0))\n",
    "\n",
    "        return results\n",
    "\n",
    "    def plot_confusion_matrix(self, y_true, y_pred, title=\"Confusion Matrix\"):\n",
    "        #plot a confusion matrix \n",
    "\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        plt.figure(figsize=(6, 5))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',xticklabels=self.class_names, yticklabels=self.class_names)\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True label')\n",
    "        plt.title(title)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def visualize_predictions(self, images, y_true, y_pred, n_samples=4):\n",
    "        \n",
    "        #Display somerandom image samples with ground truth and predicted labels, this will be useful for examples if i use them in the report\n",
    "        indices = random.sample(range(len(images)), n_samples)\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        for i, idx in enumerate(indices):\n",
    "            img = images[idx]\n",
    "            true_label = self.class_names[y_true[idx]]\n",
    "            pred_label = self.class_names[y_pred[idx]]\n",
    "            plt.subplot(1, n_samples, i + 1)\n",
    "            plt.imshow(img)\n",
    "            plt.axis('off')\n",
    "            plt.title(f\"True: {true_label}\\nPred: {pred_label}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "#initialise evaluator\n",
    "evaluator = ModelEvaluator(class_names=[\"No Mask\", \"Mask\", \"Incorrect\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## why i chose each of the metrics:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### macro \n",
    "shows how the model works on all classes equally\n",
    "### micro \n",
    "a more broad perspective, this is good for if there is an imbalance\n",
    "### weighted\n",
    " is the middle ground balancing macro with the proportions of the classes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall correctness of the model so kind of essential\n",
    "kind of limited if classes are imbalanced "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "measure for how many predicted positives were actually correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "measure how many ACTUAL postives were caught "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "combination of precision and recall, better for imbalanced classes and good to check robustness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classification report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "joined breakeddown of the previous metrics, good to find underperforming class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualsing the results so that we can see where the model confused things like incorrectly worn mask  with no mask at all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualise predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "will help with examples in the report "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  HOG + SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import hog\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# extracting HOG Features \n",
    "def extract_hog_features(images, pixels_per_cell=(8, 8), cells_per_block=(2, 2), orientations=9):\n",
    "    hog_features = []\n",
    "    for img in images:\n",
    "        # Convert to grayscale for HOG\n",
    "        gray_img = np.mean(img, axis=2) #convert to grayscale\n",
    "        #calc HOG descriptor for the image \n",
    "        features = hog(\n",
    "            gray_img,\n",
    "            orientations=orientations, #number of orientation bins \n",
    "            pixels_per_cell=pixels_per_cell, #size of the cell\n",
    "            cells_per_block=cells_per_block, #griup for local contrast\n",
    "            block_norm='L2-Hys', #method for normalization\n",
    "            feature_vector=True #make sure i output a flat vector \n",
    "        )\n",
    "        #store the descriptor\n",
    "        hog_features.append(features) \n",
    "    return np.array(hog_features) #2d array, each row is a HOG feature\n",
    "\n",
    "# training\n",
    "def train_hog_svm(X_train, y_train):\n",
    "    print(\"Extracting HOG features for training...\")\n",
    "    hog_train = extract_hog_features(X_train)\n",
    "\n",
    "    print(\"Training SVM classifier...\")\n",
    "    #rbf kernel handles non linea decision boundaries \n",
    "    clf = SVC(kernel='rbf', C=10, gamma=0.01)\n",
    "    clf.fit(hog_train, y_train)\n",
    "    return clf #the trained model\n",
    "\n",
    "# evaalujate the svm \n",
    "def evaluate_hog_svm(clf, X, y, split_name=\"Validation\"):\n",
    "    #using the validation set for now so i can see how it performs on unseen data\n",
    "    print(f\"\\n Extracting HOG features for {split_name} set...\")\n",
    "    hog_features = extract_hog_features(X)\n",
    "\n",
    "    print(f\"Predicting {split_name} set...\")\n",
    "    y_pred = clf.predict(hog_features)\n",
    "\n",
    "    # Use evaluator to get and print detailed metrics\n",
    "    evaluator.evaluate(y, y_pred, model_name=f\"HOG + SVM ({split_name})\")\n",
    "    evaluator.plot_confusion_matrix(y, y_pred, title=f\"HOG + SVM - {split_name} Confusion Matrix\")\n",
    "\n",
    "    return y_pred #the labels our model predicted\n",
    "\n",
    "# visualise\n",
    "def show_predictions(X, y_true, y_pred, n_samples=4):\n",
    "    evaluator.visualize_predictions(X, y_true, y_pred, n_samples=n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now lets run it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train on training set\n",
    "hog_svm_model = train_hog_svm(X_train, y_train)\n",
    "\n",
    "# Evaluate only on validation set\n",
    "val_preds = evaluate_hog_svm(hog_svm_model, X_val, y_val)\n",
    "\n",
    "# Visualize some results from validation set\n",
    "show_predictions(X_val, y_val, val_preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
